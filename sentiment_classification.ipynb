{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "kzKzTEt0DY95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5op2174BIql"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries:\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8zxa4BXBIqo"
      },
      "outputs": [],
      "source": [
        "# Load IMDB dataset:\n",
        "print(\"Loading IMDB dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(5000))\n",
        "dataset[\"test\"] = dataset[\"test\"].select(range(500))\n",
        "\n",
        "# Display dataset info:\n",
        "print(\"Dataset structure:\")\n",
        "print(dataset)\n",
        "print(f\"\\nTraining examples: {len(dataset['train'])}\")\n",
        "print(f\"Test examples: {len(dataset['test'])}\")\n",
        "\n",
        "# Show sample data:\n",
        "print(\"\\nSample training example:\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Text: {sample['text'][:200]}...\")\n",
        "print(f\"Label: {sample['label']} ({'positive' if sample['label'] == 1 else 'negative'})\")\n",
        "\n",
        "# Label distribution:\n",
        "train_labels = dataset['train']['label']\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "print(f\"Negative (0): {train_labels.count(0)}\")\n",
        "print(f\"Positive (1): {train_labels.count(1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d52YI6Z8BIqq"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing function:\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text data\"\"\"\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags.\n",
        "    text = text.lower()  # Convert to lowercase.\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces.\n",
        "    text = text.strip()  # Remove leading/trailing whitespaces.\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to dataset\n",
        "print(\"Preprocessing text data...\")\n",
        "dataset = dataset.map(\n",
        "    lambda x: {'text': preprocess_text(x['text']), 'label': x['label']},\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "# Show preprocessed example:\n",
        "print(\"Preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45UN4k7rBIqr"
      },
      "outputs": [],
      "source": [
        "# Model configuration:\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "# Initialize tokenizer and model:\n",
        "print(f\"Loading {MODEL_NAME} tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Tokenize datasets:\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch:\n",
        "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Tokenization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt5afvcUBIqs"
      },
      "outputs": [],
      "source": [
        "# Prepare datasets:\n",
        "train_dataset = tokenized_datasets['train']\n",
        "test_dataset = tokenized_datasets['test']\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Test examples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnXMaYI6BIqt"
      },
      "outputs": [],
      "source": [
        "# Define compute metrics function for evaluation:\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    precision = precision_score(labels, predictions, average='weighted')\n",
        "    recall = recall_score(labels, predictions, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Define training arguments:\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    seed=42,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "print(\"Training arguments set up successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87bTsBWQBIqv"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer:\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiqRsERtBIqw"
      },
      "outputs": [],
      "source": [
        "# Train the model:\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s83MrQAwBIqy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model:\n",
        "print(\"=\" * 50)\n",
        "print(\"EVALUATING MODEL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Evaluate on test set:\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"Final Test Results:\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n",
        "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
        "print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
        "print(f\"Loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# Save the model:\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model_save_path = \"./fine_tuned_sentiment_model\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "print(\"Model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDJc0Gg7BIqy"
      },
      "outputs": [],
      "source": [
        "# Test with sample predictions:\n",
        "print(\"=\" * 50)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"Predict sentiment for a given text\"\"\"\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        prediction = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "    sentiment = \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
        "    confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    return sentiment, confidence\n",
        "\n",
        "# Test with sample reviews:\n",
        "sample_reviews = [\n",
        "    \"This movie was absolutely fantastic! The acting was superb and the plot was engaging.\",\n",
        "    \"Terrible movie. I wasted my time watching this boring and poorly written film.\",\n",
        "    \"One of the best movies I've ever seen! Brilliant cinematography and outstanding performances.\"\n",
        "]\n",
        "\n",
        "for i, review in enumerate(sample_reviews, 1):\n",
        "    sentiment, confidence = predict_sentiment(review)\n",
        "    print(f\"Review {i}: {review[:50]}...\")\n",
        "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"SENTIMENT CLASSIFICATION COMPLETE!\")\n",
        "print(\"=\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}